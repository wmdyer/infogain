# infogain
using information gain to describe adj order

## using pre-trained model
```{bash}
cd data/fr
python ../../src/test.py -s triples.csv
python ../../src/regress.py -tr scores.tsv -m ig_ent
```

## source data

*CoNLLU files*

 >[CoNLL 2017 Shared Task - Automatically Annotated Raw Texts and Word Embeddings](https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-1989)  
 >[Universal Dependencies](https://github.com/UniversalDependencies)

*Word embeddings*

 >[fastText](https://fasttext.cc/docs/en/crawl-vectors.html)


## training
*1. extract NPs from conllu file*
```{bash}
./tools/extract_conllu_nps.sh <file>.conllu
```

*2. cluster (optional)*
```{bash}
cat nps.tsv | cut -f2,3 | tr '\t,' '\n' | sort -u > words
join <(cat words | sort -k1,1) <(cat <vectors> | sort -k1,1) > vecs
python ./src/cluster.py -v vecs -w words [-k <num_clusters>] [-c <pct_pca>]
```
>-v: file containing word vectors  
>-w: file containing words of interest  
>-k: number of clusters  
>-c: percentage of PCA to perform (1.0 = none)

*3. train*
```{bash}
python ./src/train.py -n nps.tsv [-c clusters.csv] [-fn 100] [-fl -1]
```
>-n: file containing NPs, generated by extract_conllu_nps.sh  
>-c: file containing clusters, generated by cluster.py; if not given, no clustering will be done  
>-fn: feature vector number--the number of attested and unattested feature vectors for each noun (-1 = unlimited)  
>-fl: feature vector length--the number of non-zero features in a vector (-1 = unlimited)

## testing

*test on AN/NA pairs*
```{bash}
./tools/extract_conllu_pairs.sh <file>.conllu
python ./src/test.py -s <file>.csv
```
>-s: scores file, generated by train.py

*test on AAN triples*
```{bash}
./tools/extract_conllu_triples.sh <file>.conllu
python ./src/test.py -s <file>.csv
```
>-s: scores file, generated by train.py

## evaluation
```{bash}
python src/regress.py -tr <scores>.tsv -m <ig_sum|ig_ent|ig_var|ig_skew> [--plot --all]
```
>-tr: training file for regression  
>-m: metric (ig_sum = sum of IGs; ig_ent = entropy of IGs; ig_var = variance of IGs; ig_skew = skewness of IGs)  
>--plot: generate plots  
>--all: compare across templates (uses TensorFlow SMOTE)  
